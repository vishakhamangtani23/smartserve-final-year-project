# -*- coding: utf-8 -*-
"""Copy of final year mini project chatbot using flask

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AY_XcVzCbjow2iKRA-flfqzDyNCi8qDl
"""

# !pip install flask flask_cors

import pandas as pd
import numpy as np

from sklearn.preprocessing import MinMaxScaler
import nltk
nltk.download('punkt')
nltk.download('stopwords')

import kagglehub

# Download latest version
path = kagglehub.dataset_download("joebeachcapital/restaurant-reviews")

print("Path to dataset files:", path)
df = pd.read_csv("C:\\Users\Vishakha mangtani\.cache\kagglehub\datasets\joebeachcapital\\restaurant-reviews\\versions\\1\Restaurant reviews.csv")
df.head()

"""# Data Cleaning"""

df = df.drop([ "Reviewer", "Metadata", "Pictures" ,"7514"], axis=1)

df["Time"] = list(map(lambda data: str(data).split()[0], df["Time"]))
df["Time"] = list(map(lambda data: str(data).split("/")[-1], df["Time"]))

df["Rating"].unique()

df["Rating"].isnull().sum()

df['Rating'].value_counts().idxmax()

df['Rating'] = np.where(df["Rating"] == "Like", df['Rating'].value_counts().idxmax(), df['Rating'])

df["Rating"].unique()

df["Rating"] = list(map(lambda data: float(data) >= 3, df["Rating"]))

df["Rating"].unique()

df["Time"].unique()

df['Time'] = np.where(df["Time"] == "nan", df['Time'].value_counts().idxmax(), df['Time'])

df["Time"].unique()

mmsTime = MinMaxScaler()

mmsTime.fit(df[["Time"]])
df["Time"] = mmsTime.transform(df[["Time"]])

df["Time"].unique()


df["Review"] = df["Review"].fillna("Nothing")

"""# NLP"""

from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
import re
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

ps = PorterStemmer()

nltk.download('punkt_tab')

reviewList = list(map(lambda data: word_tokenize(data), df["Review"]))

print(len(reviewList))

for index, word_list in enumerate(reviewList):
    reviewList[index] = " ".join([ps.stem(str(word)) for word in word_list if not word in stopwords.words("english") and word not in [".", ",", "?", "@", "$", "/"] and not word.isspace()])
print("done")
emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"
        u"\U0001F300-\U0001F5FF"
        u"\U0001F680-\U0001F6FF"
        u"\U0001F1E0-\U0001F1FF"
        u"\U00002500-\U00002BEF"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U00010000-\U0010ffff"
        u"\u2640-\u2642"
        u"\u2600-\u2B55"
        u"\u200d"
        u"\u23cf"
        u"\u23e9"
        u"\u231a"
        u"\ufe0f"
        u"\u3030"
                      "]+", re.UNICODE)

regexPuct = r"[\s\w\d]"

for index, word_list in enumerate(reviewList):
    reviewList[index] = re.sub(emoji_pattern, r'', word_list)
    reviewList[index] = "".join(re.findall(regexPuct, reviewList[index], re.MULTILINE))
    reviewList[index] = " ".join(reviewList[index].split())

df["Review"] = reviewList

df.head()

"""# Separating Data"""

X = df["Review"]
y = df["Rating"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(X_train.shape, X_test.shape)


"""# Models"""

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC

pipeline = Pipeline([
    ('vectorizer', TfidfVectorizer(ngram_range=(1,3))),
    ('classifier', SVC())
])

pipeline.fit(X_train, y_train)
pipeline.score(X_test, y_test)

"""# Testing Model"""

positive_sentence = "I absolutely love this product! It has exceeded all my expectations. It's well-designed, durable, and incredibly useful. I can't imagine my life without it now. I would highly recommend it to anyone looking for a high-quality, reliable product"
negative_sentence = "I bought this product, and it was a complete waste of money. It stopped working after just a week, and the customer service was unhelpful and rude. I will never purchase anything from this company again."

pipeline.predict([positive_sentence, negative_sentence])

def preprocess(text):
    text = word_tokenize(text)
    text = " ".join([ps.stem(str(word)) for word in text if not word in stopwords.words("english") and word not in [".", ",", "?", "@", "$", "/"] and not word.isspace()])
    text = re.sub(emoji_pattern, r'', text)
    text = "".join(re.findall(regexPuct, text, re.MULTILINE))
    text = " ".join(text.split())

    return text

positive_sentence = preprocess(positive_sentence)
negative_sentence = preprocess(negative_sentence)
print(positive_sentence)
print(negative_sentence)

pipeline.predict([positive_sentence, negative_sentence])



from fuzzywuzzy import process

# Function to recommend restaurants with enhanced query parsing
def recommend_restaurant(query):

  
    # Normalize the input query
    normalized_query = query.lower().strip()
    print(1)

    # Find reviews containing the normalized query
    relevant_reviews = df[df['Review'].str.contains(normalized_query, case=False)]

    # Sort by rating and get top 2 recommendations
    top_restaurants = relevant_reviews[relevant_reviews['Rating'] == True].head(2)

    if not top_restaurants.empty:
        return top_restaurants
    else:
        return None

# Main chatbot function with improved user input handling
def get_recommendation(user_input):
    # Normalize and clean up user input
    clean_input = user_input.lower().replace("i wish to have", "").strip()

    # If user just types "recommend", ask them to specify
    if "recommend" in clean_input:
        dish = clean_input.split("recommend")[-1].strip()
    else:
        dish = clean_input  # Directly take the cleaned input

    # Use fuzzy matching to find the closest matches in the reviews
    possible_dishes = df['Review'].str.lower().tolist()
    matched_dish = process.extractOne(dish, possible_dishes)  # Get the closest match

    if matched_dish and matched_dish[1] > 70:  # Threshold for matching confidence
        recommendations = recommend_restaurant(matched_dish[0])
    else:
        recommendations = recommend_restaurant(dish)

    # print(recommendations)  # Debugging output
    # Format response in conversational style
    if recommendations is not None:
        response = "Here are some restaurants you might like:\n"

        unique_recommendations = recommendations.drop_duplicates(subset=['Restaurant'])

        for index, row in unique_recommendations.iterrows():
            restaurant_name = row.iloc[0]
            response += f"- {restaurant_name}: This place has received great reviews!\n"
        return response
    else:
        return "I'm sorry, I couldn't find any recommendations for that."

# # Chatbot interaction
# def chatbot():
#     print("Welcome to the Restaurant Recommendation Chatbot!")
#     while True:
#         user_input = input("Ask for a restaurant recommendation (type 'exit' to quit): ")
#         if user_input.lower() == 'exit':
#             print("Goodbye!")
#             break

#         # Get recommendation response
#         response = get_recommendation(user_input)
#         print(response)

# # Run the chatbot
# chatbot()

# !pip install pyngrok
# !ngrok authtoken 2g8twh55OfVRIEAYk7USI5n0sBF_FcXD63XXvPRACaHncVUG  # Replace with your token


# public_url = ngrok.connect(5000).public_url
# print("Public URL:", public_url)


# from flask import Flask, request, jsonify
# from flask_cors import CORS

# app = Flask(__name__)
# CORS(app)  # Allow requests from frontend
# @app.route('/test',methods=['GET'])
# def foo():
#   print("hii")

# @app.route('/chatbot', methods=['POST'])
# def chatbot():
#     user_input = request.json.get('message')
#     print(user_input)
#     # while True:
#     #     user_input = input("Ask for a restaurant recommendation (type 'exit' to quit): ")
#     if user_input.lower() == 'exit':
#         print("Goodbye!")
#         return

#         # Get recommendation response
#     response = get_recommendation(user_input)

#     return jsonify({"response": response})
#     # print("Welcome to the Restaurant Recommendation Chatbot!")

#         # print(response)

# if __name__ == '__main__':
#     app.run(host="0.0.0.0", port=5000)

from flask import Flask, request, jsonify
from flask_cors import CORS  # Import CORS

app = Flask(__name__)
CORS(app)  # Enable CORS


@app.route('/test', methods=['GET'])
def foo():
    return jsonify({"message": "Hello from Flask!"})  # Added a response

@app.route('/chatbot', methods=['POST'])
def chatbot():
    data = request.get_json()  # Get JSON data safely
    print(data)
    if not data or 'message' not in data:
        return jsonify({"error": "No message provided"}), 400

    user_input = data['message']
    print(user_input)

    if user_input.lower() == 'exit':
        return jsonify({"response": "Goodbye!"})

    # Get recommendation response
    response = get_recommendation(user_input)
    return jsonify({"response": response})

if __name__ == '__main__':
    app.run(port=5000,debug=True)  # Run Flask normally
